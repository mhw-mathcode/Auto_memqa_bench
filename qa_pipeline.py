import os
import json
from typing import Any, Dict, Optional, List
from openai import OpenAI
import re
import time
import random
import argparse
from tqdm import tqdm

# å¯¼å…¥é…ç½®æ¨¡å—å’Œå„ä¸ªå¤„ç†æ¨¡å—
from config import PipelineConfig, VersionManager, build_llm_config
from src.pollution_check import pollution_check_main
from src.full_context import full_context_main
from src.new_qa import new_qa_main
from src.label import label_main

QA_GENERATE_PROMPT = """
{conversation}

Role:
You are a top-tier AI evaluation expert, specializing in designing extremely high-difficulty stress test datasets for evaluating large language modelsâ€™ long-range, cross-conversation memory.

Task:
Based on the provided long text dialogue, please design 1 high-quality question-answer pair for each of the six users {user_list} for each of the four specific categories 1 to 4 (total of {total_question_num} pairs).

Part I: Core Objectives and Depth Requirements

1. Cross-Conversation Reasoning:
- It is strictly forbidden to generate questions that can be answered using only a single session or a single utterance.
- Each question must require the model to extract and integrate information from at least two (preferably three or more) distinct conversation fragments.
- Hard-case preference: prioritize fragmented information where a clue is planted in Session A, indirectly referenced in Session B, and only revealed or resolved in Session C.

2. Extreme Source Constraints:
- Absolutely no external knowledge, common sense assumptions, associative reasoning, or hallucinations are allowed.
- If a fact is not explicitly stated or logically necessitated by the dialogue, it must be treated as non-existent.

Part II: Strict Definitions of the Four Question Categories

1. Category 1 - Long-term Persona: Examines stable identity, underlying values, long-term preferences, or behavioral patterns. It must be a consistent characteristic exhibited across multiple sessions.

2. Category 2 - Short-term State: Examines immediate emotions, short-term needs, or temporary goals in specific situations. The focus is on capturing the specific triggers that generate this state.

3. Category 3 - Temporal: Examines the absolute/relative timing of events, sequential causal relationships, or the replacement of old and new information. Requires inferring logical chains through multiple timestamps.

4. Category 4 - Plot-driven Event (Event/Experience): Examines the cause, course, and outcome of a specific experience, as well as the subjective evaluation of the participants. It must include specific actions or decisions.

Part III: De-featureization and Strong Confusion Design

1. Question Stem Design (Natural & Implicit):
- Feature leakage is forbidden. Do not use phrases such as â€œbased on their introverted personalityâ€ or â€œshows a stable coping pattern.â€
- Questions must read like natural user inquiries.
  Incorrect: â€œWhich option reflects Arielâ€™s stable breakup-coping style?â€
  Correct: â€œWhich statement best matches how Ariel dealt with the aftermath of the breakup?â€
- Language should be direct, concrete, and non-rhetorical.

2. Hard Distractor Requirements:
- Length balance (critical): the correct answer must not be the longest or shortest among the five options.
  At least one distractor must be longer than the correct answer.
- Semantic proximity: distractors must be highly plausible and lie in a high-probability semantic neighborhood.
  Avoid extreme terms such as â€œalways,â€ â€œnever,â€ â€œcompletely,â€ or â€œabsolutely.â€
- Information confusion: distractors must include
  (1) outdated statements from the target user,
  (2) true information belonging to another character (e.g., Bennett),
  (3) statements that are logically similar but factually incorrect.
- Mutual independence: options must not overlap semantically.
  No option may partially contain another optionâ€™s content.

Part IV: Structured Proof (Necessary and Sufficient Condition Validation)

1. Atomic Extraction:
- Verbatim copying only. No paraphrasing or summarization is allowed.
- Semantic completeness: if an utterance contains pronouns (e.g., â€œheâ€),
  the immediately preceding utterance that resolves the reference must also be included.
- Single-ID constraint: each evidence item must correspond to exactly one dia_id.
  Merged IDs such as â€œ1-2 1-3â€ are strictly forbidden.

2. The â€œIslandâ€ Self-Sufficiency Test:
- Logical closure: a third party reading only the evidence must be able to derive one and only one correct answer.
- No implicit knowledge: common sense or personality inference is forbidden.
  All reasoning must follow the form:
  E1 + E2 â†’ Inference
- Textual traceability: every fact used in reasoning_steps must have a direct match in evidence_dialogues.
  Logical jumps are not allowed.

3. Self-Verification Metrics:
Before outputting the final JSON, both checks must be satisfied:
- Sufficiency: are the evidence items alone sufficient to 100% eliminate all four distractors?
- Necessity (minimality): if any single evidence item is removed, does the reasoning chain break?
  Ensure no redundancy or unnecessary information.

Part V: Output JSON Specification

{{
  "qa": [
    {{
      "character": "Ariel",
      "category": 1,
      "question": "[Direct, natural, focused on the character]",
      "option": [
        "A. ",
        "B. ",
        "C. ",
        "D. ",
        "E. "
      ],
      "answer": "C",
      "evidence_dialogues": [
        {{ "id": "E1", "speaker": "Ariel", "utterance": "...", "dia_id": "" }},
        {{ "id": "E2", "speaker": "Ariel", "utterance": "...", "dia_id": "" }}
      ],
      "reasoning_steps": [
        {{
          "step": 1,
          "inference": "[Intermediate logic]",
          "based_on": ["E1"]
        }},
        {{
          "step": 2,
          "inference": "[Cross-session conclusion]",
          "based_on": ["E1", "E2"]
        }}
      ]
    }}
  ]
}}
"""


QA_GENERATE_PROMPT_2 = """
{conversation}

Role:
You are a top-tier AI evaluation expert, specializing in designing extremely high-difficulty stress test datasets for evaluating large language modelsâ€™ long-range, cross-conversation memory.

Task:
Based on the above conversation, construct questions that either focus on interpersonal relationships (such as social roles, intentions, power dynamics, or implicit emotional interactions) or require reasoning over fine-grained, specific data details within the provided context. The number of questions is flexible, but every question must be strongly aligned with its intended type and rely on inference or precise data reasoning rather than surface-level factual recall.

Part I: Core Objectives and Depth Requirements

1. Cross-Conversation Reasoning:
- It is strictly forbidden to generate questions that can be answered using only a single session or a single utterance.
- Each question must require the model to extract and integrate information from at least two (preferably three or more) distinct conversation fragments.
- Hard-case preference: prioritize fragmented information where a clue is planted in Session A, indirectly referenced in Session B, and only revealed or resolved in Session C.

2. Extreme Source Constraints:
- Absolutely no external knowledge, common sense assumptions, associative reasoning, or hallucinations are allowed.
- If a fact is not explicitly stated or logically necessitated by the dialogue, it must be treated as non-existent.

Part II: Strict Definitions of the Four Question Categories

Category 5 â€“ Interpersonal Relationship Questions

Questions that focus on explicit interpersonal relationships between individuals, such as family ties (e.g., siblings), living arrangements (e.g., roommates), educational or professional relationships (e.g., classmates, colleagues), or other clearly stated relational roles. The question must be answerable only by correctly identifying or reasoning about the concrete relationship between people, not by interpreting emotions, personalities, or abstract social norms.

Category 6 â€“ Fine-Grained Data Questions

Questions that require reasoning over explicit numerical information mentioned in the context, such as counts, dates, ages, durations, quantities, or other numeric values appearing in the narrative. All answer options should be numbers or numeric expressions, and the correct answer must depend on precise extraction, comparison, or calculation based strictly on the provided data.

Part III: De-featureization and Strong Confusion Design

1. Question Stem Design (Natural & Implicit):
- Feature leakage is forbidden. Do not use phrases such as â€œbased on their introverted personalityâ€ or â€œshows a stable coping pattern.â€
- Questions must read like natural user inquiries.
  Incorrect: â€œWhich option reflects Arielâ€™s stable breakup-coping style?â€
  Correct: â€œWhich statement best matches how Ariel dealt with the aftermath of the breakup?â€
- Language should be direct, concrete, and non-rhetorical.

2. Hard Distractor Requirements:
- Length balance (critical): the correct answer must not be the longest or shortest among the five options.
  At least one distractor must be longer than the correct answer.
- Semantic proximity: distractors must be highly plausible and lie in a high-probability semantic neighborhood.
  Avoid extreme terms such as â€œalways,â€ â€œnever,â€ â€œcompletely,â€ or â€œabsolutely.â€
- Information confusion: distractors must include
  (1) outdated statements from the target user,
  (2) true information belonging to another character (e.g., Bennett),
  (3) statements that are logically similar but factually incorrect.
- Mutual independence: options must not overlap semantically.
  No option may partially contain another optionâ€™s content.

Part IV: Structured Proof (Necessary and Sufficient Condition Validation)

1. Atomic Extraction:
- Verbatim copying only. No paraphrasing or summarization is allowed.
- Semantic completeness: if an utterance contains pronouns (e.g., â€œheâ€),
  the immediately preceding utterance that resolves the reference must also be included.
- Single-ID constraint: each evidence item must correspond to exactly one dia_id.
  Merged IDs such as â€œ1-2 1-3â€ are strictly forbidden.

2. The â€œIslandâ€ Self-Sufficiency Test:
- Logical closure: a third party reading only the evidence must be able to derive one and only one correct answer.
- No implicit knowledge: common sense or personality inference is forbidden.
  All reasoning must follow the form:
  E1 + E2 â†’ Inference
- Textual traceability: every fact used in reasoning_steps must have a direct match in evidence_dialogues.
  Logical jumps are not allowed.

3. Self-Verification Metrics:
Before outputting the final JSON, both checks must be satisfied:
- Sufficiency: are the evidence items alone sufficient to 100% eliminate all four distractors?
- Necessity (minimality): if any single evidence item is removed, does the reasoning chain break?
  Ensure no redundancy or unnecessary information.

Part V: Output JSON Specification

{{
  "qa": [
    {{
      "character": "Ariel",
      "category": 5/6,
      "question": "[Direct, natural, focused on the character]",
      "option": [
        "A. ",
        "B. ",
        "C. ",
        "D. ",
        "E. "
      ],
      "answer": "C",
      "evidence_dialogues": [
        {{ "id": "E1", "speaker": "Ariel", "utterance": "...", "dia_id": "" }},
        {{ "id": "E2", "speaker": "Ariel", "utterance": "...", "dia_id": "" }}
      ],
      "reasoning_steps": [
        {{
          "step": 1,
          "inference": "[Intermediate logic]",
          "based_on": ["E1"]
        }},
        {{
          "step": 2,
          "inference": "[Cross-session conclusion]",
          "based_on": ["E1", "E2"]
        }}
      ]
    }}
  ]
}}
"""

QA_GENERATE_PROMPT_3 = """
{conversation}

You are a top-tier AI evaluation expert specializing in designing stress test datasets to evaluate a model's ability to correctly refuse to answer when sufficient information is not available in the provided context. Your task is to construct questions that cannot be answered based solely on the provided long dialogue text. The model should be forced to 'abstain' or 'decline to answer' because the necessary information is missing, or requires external knowledge or inference beyond what is strictly present in the text.

Based on the provided long dialogue text, please design 1 question per user (Dr. Stockmann, Hovstad, Mrs. Stockmann, Peter Stockmann, Petra) for each of the four categories (1-4). This results in a total of 20 question-answer pairs.

Core Design Principle: Unanswerability
Every question must be impossible to answer definitively using only the information explicitly stated or logically entailed within the provided dialogue. Achieve this through:

Information Gap: The question asks about a fact, motive, state, or event that is never mentioned, described, or implied in any part of the dialogue.

Ambiguity/Contradiction: The dialogue contains conflicting information from different speakers or sessions about the key point of the question, making a single definitive answer impossible.

Requires External Knowledge: Answering correctly would require common sense, real-world knowledge, or associative reasoning that is not contained within the dialogue's text.

Temporal Impossibility: The question asks about an event clearly stated to happen after the dialogue's end or before its beginning, with no details given in the text.

Strict Adherence to Category Definitions (For Question Construction Only)
Construct questions that appear to fit these categories, but whose answers are unattainable.

Category 1 - Long-term Persona: Ask about a stable trait, value, or pattern that is not demonstrated or discussed across the sessions.

Category 2 - Short-term State: Ask about a specific emotion, need, or goal in a precise moment that is not revealed by the character's words or actions in the text.

Category 3 - Temporal: Ask about the timing, sequence, or duration of events that are not specified or are contradicted within the dialogue.

Category 4 - Plot-driven Event: Ask about the cause, detail, or outcome of a specific experience/action that is not described in the dialogue.

Question & Distractor Design for Unanswerability
Question Stem: Must sound natural and plausible, as if it could be answered by a knowledgeable reader.

Options: Include five options (A-E).

FIve Distractors: Each should be a plausible-sounding but incorrect statement. They should be constructed from out-of-context quotes, misattributed actions, or logical leaps not supported by the text.

One Correct Answer: F. Cannot infer the answer based on the given information.

Distractor Quality: Distractors should be semantically close to topics in the dialogue to create confusion, but must not be verifiably true based solely on the text.

Evidence & Reasoning Requirements (Proving Unanswerability)
evidence_dialogues: Provide 2-4 key dialogue snippets that are most relevant to the question's topic. These snippets should demonstrate that the information needed to answer is absent, vague, or contradictory. It is acceptable if the evidence shows a character discussing the topic without providing the asked-for information.

reasoning_steps: The logical steps must prove why no option can be confirmed as true and why the correct answer is to abstain.

Output Format

Output a valid JSON list named "qa" containing 20 objects.

Step 1: State what the question is asking for.

Step 2: Analyze the provided evidence to show the information gap, contradiction, or external knowledge requirement.

Step 3: Conclude that all distractors are unverifiable or false, and the only valid response is to decline to answer.

{{
  "qa": [
    {{
      "character": "Dr. Stockmann",
      "category": 1,
      "question": "[A natural-sounding question about the character that cannot be answered]",
      "options": [
        "A. [Plausible but unverifiable/distractor statement 1]",
        "B. [Plausible but unverifiable/distractor statement 2]",
        "C. [Plausible but unverifiable/distractor statement 3]",
        "D. [Plausible but unverifiable/distractor statement 4]",
        "E. [Plausible but unverifiable/distractor statement 5]",
        "F. Cannot infer the answer based on the given information."
      ],
      "answer": "F",
      "evidence_dialogues": [
        {{"id": "E1", "speaker": "X", "utterance": "...", "dia_id": "X-Y"}},
        {{"id": "E2", "speaker": "Y", "utterance": "...", "dia_id": "X-Z"}}
      ],
      "reasoning_steps": [
        {{
          "step": 1,
          "inference": "The question asks for [specific information X].",
          "based_on": []
        }},
        {{
          "step": 2,
          "inference": "The provided dialogues show characters discussing related topic Y, but never address or specify X. / The dialogues present conflicting views on X between Speaker A and Speaker B.",
          "based_on": ["E1", "E2"]
        }},
        {{
          "step": 3,
          "inference": "Options A-D make claims that are either contradicted by the text, attributed to the wrong character, or require assumptions beyond the text. Therefore, the only supportable conclusion is that the information is unavailable.",
          "based_on": ["E1", "E2"]
        }}
      ]
    }}
  ]
}}
"""

def call_openai_json(
    answer_prompt: str,
    model: str,
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    timeout_s: int = 120
) -> Dict[str, Any]:
    """
    Invoke the large model to generate a JSON file
    """
    if api_key is None:
        api_key = os.environ.get("OPENAI_API_KEY", "")
    if base_url is None:
        base_url = os.environ.get("OPENAI_BASE_URL", None)

    client = OpenAI(
        api_key=api_key,
        base_url=base_url
    )

    # -------- strict json helpers --------
    def _strip_code_fences(text: str) -> str:
        t = (text or "").strip()
        if t.startswith("```"):
            # remove opening fence: ``` or ```json
            t = re.sub(r"^\s*```(?:json)?\s*\n?", "", t, flags=re.IGNORECASE)
            # remove closing fence
            t = re.sub(r"\n?\s*```\s*$", "", t)
            t = t.strip()
        return t

    def _strict_json_loads(text: str) -> Dict[str, Any]:
        t = _strip_code_fences(text)
        obj = json.loads(t)  # strict: must parse directly
        if not isinstance(obj, dict):
            raise ValueError(f"Top-level JSON must be an object/dict, got {type(obj)}")
        return obj

    # -------- retry loop --------
    max_retries = 10  # json ç”Ÿæˆå¤±è´¥é‡è¯•æ¬¡æ•°
    last_content = ""

    for attempt in range(max_retries + 1):
        MAX_OTHER_ERROR_RETRIES = 10 # å…¶ä»–è¯·æ±‚å¤±è´¥é‡è¯•æ¬¡æ•°
        llm_error_retries = 0
        other_error_retries = 0
        resp = None
        while True:
            try:
                resp = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": answer_prompt}
                    ],
                    extra_body={
                        "enable_thinking": False
                    }
                )
                break
            except Exception as e:
                # --- æ‰“å°å®Œæ•´ Traceback ---
                # print(f"\n[ERROR] Attempt {attempt} failed for model {model}:")
                # traceback.print_exc() # è¿™è¡Œä¼šæ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆï¼ŒåŒ…æ‹¬é”™è¯¯å‘ç”Ÿçš„è¡Œå·
                error_str = str(e).lower()
                print(error_str)
                if "rate limit" in error_str or "limit" in error_str or "overloaded" in error_str or "token" in error_str:
                    # è¯†åˆ«ä¸º TPM åˆ™ä¸€ç›´é‡è¯•
                    llm_error_retries += 1
                    other_error_retries = 0 # é‡ç½®å…¶ä»–é”™è¯¯è®¡æ•°
                    sleep_duration = random.uniform(2, 20) + 5 * llm_error_retries
                    error_message = f"LLM Rate Limit related Error. Retrying in {sleep_duration:.2f}s... Error: {e}"
                    print(error_message)
                    time.sleep(sleep_duration)
                else:
                    # è¯†åˆ«ä¸ºå…¶ä»–é”™è¯¯
                    other_error_retries += 1
                    print("other_error_retries: ", other_error_retries)
                    if other_error_retries >= MAX_OTHER_ERROR_RETRIES:
                        response_content = "Error: Default response due to unrecoverable error." # è®¾ç½®é»˜è®¤å€¼
                        print(response_content)
                        break # è¾¾åˆ°æœ€å¤§æ¬¡æ•°ï¼Œè·³å‡ºå¾ªçŽ¯

        last_content = resp.choices[0].message.content or ""

        try:
            return _strict_json_loads(last_content)
        except Exception:
            # invalid json -> retry
            continue

    # if still invalid after retries
    raise ValueError(
        f"Judge did not return valid JSON after {max_retries + 1} attempts.\n"
        f"Last output:\n{last_content}"
    )


def main():
    """
    é—®ç­”æ•°æ®é›†å¤„ç†ä¸»æµç¨‹
    
    æ•°æ®ç»“æž„è¯´æ˜Žï¼š
    - æ‰€æœ‰ç‰ˆæœ¬éƒ½ä½¿ç”¨ç»Ÿä¸€çš„åˆ—è¡¨æ ¼å¼ï¼š
      [
        {
          "filename": "åŽŸå§‹æ–‡ä»¶å",
          "conversation": {...å¯¹è¯å†…å®¹...},
          "qa": [...é—®ç­”åˆ—è¡¨...]
        },
        ...
      ]
    
    æµç¨‹è¯´æ˜Žï¼š
    v0: åŽŸå§‹ç”Ÿæˆçš„é—®ç­”å¯¹ï¼ˆæ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªåˆ—è¡¨å…ƒç´ ï¼‰
    v1: æ‰“ä¹±é€‰é¡¹å’Œæ±¡æŸ“æ£€æŸ¥åŽçš„æ•°æ®
    v2: å…¨ä¸Šä¸‹æ–‡éªŒè¯åŽçš„æ•°æ®ï¼ˆä¿ç•™ç­”å¯¹çš„é¢˜ç›®ï¼‰
    v3: æ ‡æ³¨åˆ†ç±»åŽçš„æ•°æ®
    v4: ç²¾ç‚¼é‡æž„åŽçš„æœ€ç»ˆé—®ç­”
    """
    parser = argparse.ArgumentParser(description="QAæ•°æ®é›†å®Œæ•´å¤„ç†æµç¨‹")
    
    # åŸºç¡€é…ç½®
    parser.add_argument("--dataset_name", required=True, help="æ•°æ®é›†åç§°")
    parser.add_argument("--input_dir", default="dataset", help="è¾“å…¥æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", default="result", help="è¾“å‡ºç»“æžœç›®å½•")
    parser.add_argument("--temp_dir", default="temp", help="ä¸´æ—¶æ–‡ä»¶ç›®å½•")

    # QAç”Ÿæˆæ¨¡åž‹é…ç½®
    parser.add_argument("--qa_llm_model", default="Qwen/Qwen3-14B")
    parser.add_argument("--qa_llm_base_url", default="")
    parser.add_argument("--qa_llm_api_key", default="")

    # ç­”æ¡ˆéªŒè¯æ¨¡åž‹é…ç½®
    parser.add_argument("--answer_llm_model", default="Qwen/Qwen3-14B")
    parser.add_argument("--answer_llm_base_url", default="")
    parser.add_argument("--answer_llm_api_key", default="")
    
    # å¹¶å‘é…ç½®
    parser.add_argument("--max_workers", type=int, default=4, help="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°")
    
    # æµç¨‹æŽ§åˆ¶ï¼ˆå¯é€‰æ‹©æ€§è¿è¡ŒæŸäº›æ­¥éª¤ï¼‰
    parser.add_argument("--skip_generation", action="store_true", help="è·³è¿‡é—®ç­”ç”Ÿæˆæ­¥éª¤")
    parser.add_argument("--skip_pollution", action="store_true", help="è·³è¿‡æ±¡æŸ“æ£€æŸ¥æ­¥éª¤")
    parser.add_argument("--skip_full_context", action="store_true", help="è·³è¿‡å…¨ä¸Šä¸‹æ–‡éªŒè¯æ­¥éª¤")
    parser.add_argument("--skip_label", action="store_true", help="è·³è¿‡æ ‡æ³¨åˆ†ç±»æ­¥éª¤")
    parser.add_argument("--skip_refine", action="store_true", help="è·³è¿‡ç²¾ç‚¼é‡æž„æ­¥éª¤")
    
    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    config = PipelineConfig(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        temp_dir=args.temp_dir,
        qa_llm=build_llm_config(args.qa_llm_model, args.qa_llm_base_url, args.qa_llm_api_key),
        answer_llm=build_llm_config(args.answer_llm_model, args.answer_llm_base_url, args.answer_llm_api_key),
        max_workers=args.max_workers
    )
    
    version_manager = VersionManager(config)
    version_manager.print_version_info()
    
    print("\n" + "="*70)
    print(f"ðŸ“š å¼€å§‹å¤„ç†æ•°æ®é›†: {args.dataset_name}")
    print("="*70)
    
    # ============================================================
    # æ­¥éª¤ 0: ç”ŸæˆåŽŸå§‹é—®ç­”å¯¹ (v0)
    # ============================================================
    v0_path = version_manager.get_path("v0", args.dataset_name)
    
    if not args.skip_generation:
        print("\n" + "="*70)
        print("ðŸ”„ æ­¥éª¤ 0: ç”ŸæˆåŽŸå§‹é—®ç­”å¯¹")
        print("="*70)
        
        all_data = []  # æ”¹ä¸ºå­˜å‚¨åŒ…å« conversation å’Œ qa çš„å®Œæ•´æ•°æ®
        total_qa_count = 0
        
        dataset_dir = os.path.join(args.input_dir, args.dataset_name)

        json_files = sorted(
            f for f in os.listdir(dataset_dir)
            if f.endswith(".json")
        )
		
        for filename in tqdm(json_files, desc=f"å¤„ç† {args.dataset_name}"):
            file_path = os.path.join(dataset_dir, filename)

            if not os.path.exists(file_path):
                print(f"[SKIP] {file_path} ä¸å­˜åœ¨")
                continue
            
            # åŠ è½½å¯¹è¯æ•°æ®
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            if isinstance(data, list):
                conversation = data[0].get("conversation", []) if data else []
            elif isinstance(data, dict):
                conversation = data.get("conversation", [])
            else:
                raise TypeError(f"Unsupported data type: {type(data)}")

            if not conversation:
                print(f"[SKIP] {file_path} å¯¹è¯ä¸ºç©º")
                continue
            
            speakers = conversation.get("speakers", [])
            print(f"\nå¤„ç†æ–‡ä»¶ {file_path}, è¯´è¯è€…: {speakers}")
            
            # ç”Ÿæˆç±»åˆ« 1-4 çš„é—®é¢˜
            answer_prompt = QA_GENERATE_PROMPT.format(
                conversation=conversation,
                user_list=speakers,
                question_num=1,
                total_question_num=len(speakers) * 4
            )
            
            all_question = call_openai_json(
                answer_prompt=answer_prompt,
                model=args.qa_llm_model,
                api_key=args.qa_llm_api_key,
                base_url=args.qa_llm_base_url
            )
            
            # ç”Ÿæˆç±»åˆ« 5-6 çš„é—®é¢˜
            answer_prompt_2 = QA_GENERATE_PROMPT_2.format(
                conversation=conversation,
            )
            
            all_question_2 = call_openai_json(
                answer_prompt=answer_prompt_2,
                model=args.qa_llm_model,
                api_key=args.qa_llm_api_key,
                base_url=args.qa_llm_base_url
            )
            
            # æ±‡æ€»å½“å‰æ–‡ä»¶çš„é—®é¢˜
            current_qa = []
            if "qa" in all_question and isinstance(all_question["qa"], list):
                current_qa.extend(all_question["qa"])
                print(f"  ç”Ÿæˆç±»åˆ«1-4é—®é¢˜: {len(all_question['qa'])} ä¸ª")
            
            if "qa" in all_question_2 and isinstance(all_question_2["qa"], list):
                current_qa.extend(all_question_2["qa"])
                print(f"  ç”Ÿæˆç±»åˆ«5-6é—®é¢˜: {len(all_question_2['qa'])} ä¸ª")
            
            # å°† conversation å’Œ qa æ‰“åŒ…æˆä¸€ä¸ª dict
            file_data = {
                "filename": filename,
                "conversation": conversation,
                "qa": current_qa
            }
            all_data.append(file_data)
            total_qa_count += len(current_qa)
        
        # ä¿å­˜ v0 ç‰ˆæœ¬ - çŽ°åœ¨æ˜¯ä¸€ä¸ª list
        with open(v0_path, "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… v0 ç‰ˆæœ¬å·²ä¿å­˜: {v0_path}")
        print(f"   å…±ç”Ÿæˆ {len(all_data)} ä¸ªæ–‡ä»¶çš„æ•°æ®")
        print(f"   å…±ç”Ÿæˆ {total_qa_count} ä¸ªé—®ç­”å¯¹")
    else:
        print(f"\nâ© è·³è¿‡é—®ç­”ç”Ÿæˆæ­¥éª¤ï¼Œä½¿ç”¨çŽ°æœ‰æ–‡ä»¶: {v0_path}")
    
    # ============================================================
    # æ­¥éª¤ 1: æ•°æ®æ±¡æŸ“æ£€æŸ¥ (v0 -> v1)
    # ============================================================
    v1_path = version_manager.get_path("v1", args.dataset_name)
    
    if not args.skip_pollution:
        v1_path = pollution_check_main(args, v0_path, v1_path)
    else:
        print(f"\nâ© è·³è¿‡æ±¡æŸ“æ£€æŸ¥æ­¥éª¤ï¼Œä½¿ç”¨çŽ°æœ‰æ–‡ä»¶: {v1_path}")
    
    # ============================================================
    # æ­¥éª¤ 2: å…¨ä¸Šä¸‹æ–‡éªŒè¯ - ä¸¤æ¬¡æ£€æŸ¥ (v1 -> v2a -> v2b)
    # ============================================================
    v2a_path = version_manager.get_path("v2a", args.dataset_name)
    v2b_path = version_manager.get_path("v2b", args.dataset_name)
    
    if not args.skip_full_context:
        print("\n" + "="*70)
        print("ðŸ”„ æ­¥éª¤ 2: å…¨ä¸Šä¸‹æ–‡éªŒè¯ï¼ˆä¸¤é˜¶æ®µç­›é€‰ï¼‰")
        print("="*70)
        
        # ç¬¬ä¸€æ¬¡æ£€æŸ¥ï¼šonly_evidence=1ï¼Œä¿ç•™åªçœ‹è¯æ®å°±èƒ½ç­”å¯¹çš„é¢˜ç›®
        print("\nðŸ“ ç¬¬ä¸€é˜¶æ®µï¼šä¿ç•™åŸºäºŽè¯æ®ç­”å¯¹çš„é¢˜ç›®")
        v2a_path, kept_count_a = full_context_main(
            args=args,
            input_file_path=v1_path,
            output_file_path=v2a_path,
            only_evidence=1,
            except_evidence=0
        )
        print(f"   âœ“ ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œä¿ç•™é¢˜ç›®: {kept_count_a} ä¸ª")
        
        # ç¬¬äºŒæ¬¡æ£€æŸ¥ï¼šexcept_evidence=1ï¼Œç­›æŽ‰ä¸çœ‹è¯æ®å°±èƒ½ç­”å¯¹çš„é¢˜ç›®
        print("\nðŸ“ ç¬¬äºŒé˜¶æ®µï¼šç­›æŽ‰ä¸çœ‹è¯æ®ä¹Ÿèƒ½ç­”å¯¹çš„é¢˜ç›®")
        v2b_path, kept_count_b = full_context_main(
            args=args,
            input_file_path=v2a_path,
            output_file_path=v2b_path,
            only_evidence=0,
            except_evidence=1
        )
        print(f"   âœ“ ç¬¬äºŒé˜¶æ®µå®Œæˆï¼Œæœ€ç»ˆä¿ç•™é¢˜ç›®: {kept_count_b} ä¸ª")
        
        # ä½¿ç”¨ç¬¬äºŒé˜¶æ®µçš„è¾“å‡ºä½œä¸ºæœ€ç»ˆçš„v2
        v2_path = v2b_path
        
        print("\n" + "="*70)
        print(f"ðŸ“Š éªŒè¯ç»Ÿè®¡ï¼šåˆå§‹ â†’ {kept_count_a} â†’ {kept_count_b}")
        print("="*70)
    else:
        print(f"\nâ© è·³è¿‡å…¨ä¸Šä¸‹æ–‡éªŒè¯æ­¥éª¤ï¼Œä½¿ç”¨çŽ°æœ‰æ–‡ä»¶: {v2b_path}")
        v2_path = v2b_path
    
    # ============================================================
    # æ­¥éª¤ 3: é—®é¢˜åˆ†ç±»æ ‡æ³¨ (v2 -> v3)
    # ============================================================
    v3_path = version_manager.get_path("v3", args.dataset_name)
    
    if not args.skip_label:
        v3_path = label_main(
            input_file_path=v2_path,
            output_file_path=v3_path,
            api_key=args.answer_llm_api_key,
            base_url=args.answer_llm_base_url,
            model_name=args.answer_llm_model
        )
    else:
        print(f"\nâ© è·³è¿‡æ ‡æ³¨åˆ†ç±»æ­¥éª¤ï¼Œä½¿ç”¨çŽ°æœ‰æ–‡ä»¶: {v3_path}")
    
    # ============================================================
    # æ­¥éª¤ 4: é—®ç­”ç²¾ç‚¼é‡æž„ (v3 -> v4)
    # ============================================================
    v4_path = version_manager.get_path("v4", args.dataset_name)
    final_output = os.path.join(args.output_dir, f"{args.dataset_name}_final.json")
    
    if not args.skip_refine:
        v4_path = new_qa_main(
            input_file_path=v3_path,
            output_file_path=v4_path,
            api_key=args.qa_llm_api_key,
            base_url=args.qa_llm_base_url,
            model=args.qa_llm_model
        )
        
        # å¤åˆ¶æœ€ç»ˆç‰ˆæœ¬åˆ°è¾“å‡ºç›®å½•
        import shutil
        shutil.copy(v4_path, final_output)
        print(f"\nðŸ“¦ æœ€ç»ˆç‰ˆæœ¬å·²å¤åˆ¶åˆ°: {final_output}")
    else:
        print(f"\nâ© è·³è¿‡ç²¾ç‚¼é‡æž„æ­¥éª¤ï¼Œä½¿ç”¨çŽ°æœ‰æ–‡ä»¶: {v4_path}")
    
    # ============================================================
    # å®Œæˆæ€»ç»“
    # ============================================================
    print("\n" + "="*70)
    print("ðŸŽ‰ æ•°æ®å¤„ç†æµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print("="*70)
    print("\nðŸ“ ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print(f"  v0 (åŽŸå§‹):     {v0_path}")
    print(f"  v1 (æ‰“ä¹±):     {v1_path}")
    print(f"  v2 (éªŒè¯):     {v2_path}")
    print(f"  v3 (æ ‡æ³¨):     {v3_path}")
    print(f"  v4 (é‡æž„):     {v4_path}")
    print(f"  æœ€ç»ˆè¾“å‡º:      {final_output}")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()


# python qa_pipeline.py --dataset_name An-Enemy-of-the-People --skip_generation --skip_pollution --skip_full_context --skip_label --qa_llm_model qwen3-14b --qa_llm_base_url https://api.vveai.com/v1 --qa_llm_api_key UQ3rMy9zeMTzD4AAF83eB5F4EcE84d6d9170CcB56a43F8F3 --answer_llm_model qwen3-14b --answer_llm_base_url https://api.vveai.com/v1 --answer_llm_api_key UQ3rMy9zeMTzD4AAF83eB5F4EcE84d6d9170CcB56a43F8F3 --max_workers 4 > pipeline.log 2>&1

