import json
import os
import re
import numpy as np
from collections import defaultdict, OrderedDict
from openai import OpenAI
from sentence_transformers import SentenceTransformer, util

DEFAULT_API_KEY = os.getenv("OPENAI_API_KEY", "")
DEFAULT_BASE_URL = os.getenv("OPENAI_BASE_URL", "")
DEFAULT_MODEL = os.getenv("NEW_QA_MODEL", "Qwen/Qwen3-14B")
DEFAULT_EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "paraphrase-multilingual-MiniLM-L12-v2")
DEFAULT_CLUSTERING_THRESHOLD = float(os.getenv("CLUSTERING_THRESHOLD", "0.55"))

BASE_URL = DEFAULT_BASE_URL
API_KEY = DEFAULT_API_KEY
MODEL = DEFAULT_MODEL
CLUSTERING_THRESHOLD = DEFAULT_CLUSTERING_THRESHOLD

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
embedder = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)

def gen_chat(prompt: str, temp=0.7) -> str:
    try:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=temp,
            extra_body={"enable_thinking": False},
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"!!! è°ƒç”¨æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return ""

# ================= æ ¸å¿ƒå¤„ç†ç±» =================

class UltimateMemoryRefiner:
    def __init__(self, input_file, output_file):
        self.input_file = input_file
        self.output_file = output_file
        self.raw_data = []
        self.original_data = []  # ä¿å­˜åŸå§‹çš„å®Œæ•´æ•°æ®ç»“æ„

    def load_data(self):
        """åŠ è½½æ•°æ®å¹¶æ ‡è®° episode_index"""
        with open(self.input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å¦‚æœæ˜¯å•ä¸ª dictï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        if isinstance(data, dict):
            data = [data]
        
        # ä¿å­˜åŸå§‹æ•°æ®ç»“æ„
        self.original_data = data
        
        # æå–æ‰€æœ‰ QA å¹¶æ·»åŠ  qidï¼ˆå…¨å±€å”¯ä¸€IDï¼‰ã€episode_index å’Œ source_index
        qid = 1
        for idx, item in enumerate(data):
            if "qa" in item:
                for qa_item in item["qa"]:
                    # å¦‚æœåŸæ¥æ²¡æœ‰ qidï¼Œç”Ÿæˆä¸€ä¸ª
                    if "qid" not in qa_item:
                        qa_item["qid"] = qid
                        qid += 1
                    qa_item["episode_index"] = idx
                    qa_item["source_index"] = idx  # æ ‡è®°æ¥æºitem
                    self.raw_data.append(qa_item)
        
        print(f"--- [Step 1] æ•°æ®åŠ è½½å®Œæˆï¼Œå…±è®¡ {len(self.raw_data)} æ¡åŸå§‹ QAï¼Œæ¥è‡ª {len(data)} ä¸ªæ•°æ®æº ---")

    def semantic_clustering(self, qa_list, threshold=None):
        """è¯­ä¹‰èšç±»å¹¶è¾“å‡ºæœ€å¤§ç°‡ Size"""
        if threshold is None:
            threshold = CLUSTERING_THRESHOLD
        
        sentences = [q["question"] for q in qa_list]
        embeddings = embedder.encode(sentences, convert_to_tensor=True)
        
        # ç¤¾åŒºæ£€æµ‹ç®—æ³•
        clusters = util.community_detection(embeddings, min_community_size=2, threshold=threshold)
        
        if not clusters:
            return [[qa] for qa in qa_list], 1
        
        max_cluster_size = max(len(c) for c in clusters)
        
        clustered_data = []
        assigned_indices = set()
        for cluster in clusters:
            clustered_data.append([qa_list[idx] for idx in cluster])
            assigned_indices.update(cluster)
            
        remaining = [qa_list[i] for i in range(len(qa_list)) if i not in assigned_indices]
        for r in remaining: clustered_data.append([r])
            
        return clustered_data, max_cluster_size

    def scan_cluster_logic(self, subject, cluster):
        """
        [æ–°å¢ç¯èŠ‚]ï¼šä½¿ç”¨ LLM æ‰«æèšç±»ç»“æœï¼Œåˆ¤æ–­å…¶æ˜¯å¦å…·å¤‡â€œé‡æ„ä»·å€¼â€
        """
        if len(cluster) < 2: return cluster # å­¤ç«‹ç‚¹ä¸æ‰«æ
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé€»è¾‘åˆ†æå¸ˆã€‚è¯·å®¡è§†ä»¥ä¸‹å…³äºå®ä½“ã€{subject}ã€‘çš„è¯­ä¹‰ç›¸å…³é—®é¢˜ç°‡ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. è¯†åˆ«è¿™äº›é—®é¢˜æ˜¯å¦å›´ç»•åŒä¸€ä¸ªå±æ€§æˆ–äº‹ä»¶å±•å¼€ã€‚
2. é‡ç‚¹å¯»æ‰¾éšåºåˆ—å·(episode_index)å¢åŠ è€Œå‘ç”Ÿçš„çŠ¶æ€å†²çªï¼ˆè®°å¿†æ›´æ–°ç‚¹ï¼‰æˆ–ä¿¡æ¯äº’è¡¥ï¼ˆé€»è¾‘æ•´åˆç‚¹ï¼‰ã€‚
3. å¦‚æœè¿™ç»„æ•°æ®é€»è¾‘æ··ä¹±æˆ–æ— å…³è”ï¼Œè¯·è¿”å› "REJECT"ï¼Œå¦åˆ™è¿”å›ç®€çŸ­çš„é€»è¾‘æ¼”å˜æè¿°ã€‚

### æ•°æ®ç°‡ï¼š
{json.dumps(cluster, ensure_ascii=False, indent=2)}
"""
        res = gen_chat(prompt, temp=0.3)
        return None if "REJECT" in res else cluster

    def build_refine_prompt(self, subject, final_chunk):
        """æœ€ç»ˆç”Ÿæˆ Prompt"""
        return f"""
# Task Description
You are a high-difficulty long-context logical evaluation and question generation system.

You will be given a set of original QA data about a specific entity {subject}.
All data is ordered temporally, but information is distributed across multiple non-contiguous semantic chunks.

Your task is to generate a set of challenging logical evaluation questions that assess:
- Long-term memory update ability
- Cross-chunk logical integration ability

All questions must be derived strictly from the provided data.
They must require temporal reasoning, state comparison, conflict resolution, and multi-evidence integration, rather than surface-level paraphrasing.

# Dimension 1: Memory Update

When an entityâ€™s state S is A at time t1 and is later explicitly or implicitly updated to B at time t2, you must construct questions around this state transition, including but not limited to:

1. Causal (Why-based)
   - Ask why state A became invalid
   - Ask which explicitly mentioned events, decisions, or conditions caused or enabled the transition to state B

2. Boundary / Timing
   - Ask for the specific point at which the old state was irreversibly overturned
   - This point is not necessarily the first anomaly, but when the update became final

3. Final-State Verification
   - Explicitly include early state A as a strong distractor in the question
   - Ask about the entityâ€™s final state at the end of the full data sequence
   - Designed to detect reliance on outdated memory

For each subtype above:
If multiple updates, reversals, or influencing factors exist in the data, you should generate multiple questions from different analytical perspectives, not just a single question.


# Dimension 2: Integrated Logic Across Chunks

You must actively identify related facts or patterns distributed across multiple non-adjacent semantic chunks and construct questions that require joint reasoning across them, including but not limited to:

1. Set Construction / Inductive Aggregation
   - Ask the model to enumerate all moments or behaviors matching
     an abstract property
   - The property must not be explicitly summarized in any single chunk

2. Trend / Frequency Analysis
   - Ask whether a behavior, attitude, or decision pattern changes over time
   - Changes may involve escalation, attenuation, or structural shifts

3. Multi-Chunk Dependency (Fragment Assembly)
   - The correct answer must depend on information from at least two different semantic chunks
   - Missing any chunk should lead to an incomplete or incorrect answer

The same cross-chunk pattern may be queried from multiple angles, and multiple questions should be generated when appropriate.


# Mandatory Constraints

1. No External Knowledge
   - All questions, options, and answers must be based exclusively on the provided data
   - No background knowledge, common sense completion, or assumptions allowed

2. No Meta-Context References
   - Do not mention â€œepisodesâ€, â€œchaptersâ€, â€œearlier textâ€, or similar notions
   - Treat the input strictly as a complete and standalone data sequence

3. Implicit Reasoning Requirement
   - Questions must implicitly require deep reasoning
   - The model is NOT required to expose reasoning in its answer


# Option Construction Constraints

1. Each question must provide multiple options (e.g., A / B / C / D / E).

2. Incorrect options must be plausible but unambiguously wrong:
   - They must be partially supported by the text
   - But invalidated by later updates or cross-chunk evidence

3. Common sources of incorrect options include:
   - Reliance on early states while ignoring updates
   - Use of a single chunk while ignoring others
   - Confusing correlation with causation

4. The correct option must not be obtainable via keyword matching alone;
   it must require temporal ordering, state comparison, or evidence integration.


# Answer Field Constraints

1. The `answer` field must contain ONLY the final conclusion:
   - e.g., the correct option letter, final state, key timestamp, or final set
   - No explanation, justification, or reasoning text

2. The answer must be unique and deterministic.
   Ambiguous or multi-valid answers are not allowed.


# Explanation / Reasoning Field Constraints

1. The `reasoning` (or `explanation`) field must document:
   - The logical basis for the correct answer
   - How memory updates occurred
   - How information from multiple chunks was integrated
   - How conflicts were identified and resolved

2. This field exists for:
   - Annotation quality control
   - Debugging and error analysis
   - Benchmark interpretability

3. Every key claim in the reasoning must be traceable
   to a specific semantic chunk or time point.

4. "category": The question type label, which must remain consistent with the category of the selected original question(s) from which this item is constructed.

5. "original_qa_qid": A list of qid values corresponding to the original QA items that were selected, referenced, or integrated to construct the current question.

# Input Semantic Cluster
{json.dumps(final_chunk, ensure_ascii=False, indent=4)}


# Output Format (JSON)
[
    {{
        "question": "Complete question text with necessary distractors",
        "option": ["A ...", "B ...", "C ...", "D ...", "E ..."],
        "answer": "Final conclusion or correct option label",
        "reasoning": "Detailed explanation of how the answer follows from the full data",
        "label": "memory_update / integrated_logic",
        "category": 1,
        "evidence_chunks": [0, 2, 5],
        "is_conflict": true / false,
        "original_qa": ["Referenced original question(s) or summaries"],
        "original_qa_qid": [1, 4] 
    }}
]

"""

    def process(self):
        """
        å¤„ç†æµç¨‹ï¼šå¯¹æ‰€æœ‰é—®é¢˜è¿›è¡Œå…¨å±€èšç±»åˆ†æå’Œé‡æ„
        """
        self.load_data()
        
        print(f"\n>>> å¼€å§‹å…¨å±€èšç±»åˆ†æå’Œé‡æ„å¤„ç†")
        
        # æŒ‰è§’è‰²åˆ†ç»„ï¼ˆå…¨å±€ï¼‰
        subject_buckets = defaultdict(list)
        for qa in self.raw_data:
            subject_buckets[qa.get("character", "Unknown")].append(qa)
        
        # å­˜å‚¨æ‰€æœ‰é‡æ„åçš„æ–°é—®é¢˜
        all_refined_qa = []
        
        for subject, subject_qa_list in subject_buckets.items():
            if subject == "Unknown": 
                continue
            
            print(f"\n>>> æ­£åœ¨å¤„ç†è§’è‰²ã€{subject}ã€‘(å…± {len(subject_qa_list)} ä¸ªé—®é¢˜)...")
            
            # è¯­ä¹‰èšç±»
            semantic_groups, max_size = self.semantic_clustering(subject_qa_list)
            print(f"   âˆš è¯­ä¹‰èšç±»å®Œæˆã€‚æœ€å¤§ç°‡å¤§å°: {max_size}")
            
            # LLM é€»è¾‘å®¡æ ¡å’Œé‡æ„
            print(f"   æ­£åœ¨è¿›è¡Œ LLM é€»è¾‘å®¡æ ¡ (å…± {len(semantic_groups)} ä¸ªè¯é¢˜ç°‡)...")
            for i, group in enumerate(semantic_groups):
                if len(group) < 2:
                    # å­¤ç«‹ç‚¹è·³è¿‡ï¼Œä¸ç”Ÿæˆæ–°é—®é¢˜
                    print(f"      - ç°‡ {i} ä¸ºå­¤ç«‹ç‚¹ï¼Œè·³è¿‡")
                    continue
                
                passed_group = self.scan_cluster_logic(subject, group)
                if not passed_group:
                    print(f"      - ç°‡ {i} è¢«é€»è¾‘å®¡æ ¡æ‹’ç» (æ— å†²çªæˆ–å…³è”)ï¼Œè·³è¿‡")
                    continue
                
                # ç”Ÿæˆé‡æ„åçš„é—®é¢˜
                refined = None
                max_retries = 10
                
                for attempt in range(max_retries):
                    current_prompt = self.build_refine_prompt(subject, passed_group)
                    if attempt > 0:
                        current_prompt += "\n\n**é‡è¦ä¿®æ­£**ï¼šè¯·ç›´æ¥è¾“å‡º JSON æ•°ç»„æ ¼å¼ï¼ˆä»¥ [ å¼€å¤´ï¼Œä»¥ ] ç»“æŸï¼‰ï¼Œä¸¥ç¦åŒ…å«ä»»ä½• Markdown ä»£ç å—æ ‡ç­¾ã€å‰è¨€ã€è§£é‡Šæˆ–ç»“å°¾æ€»ç»“ã€‚"
                    
                    response = gen_chat(current_prompt)
                    refined = self.extract_json(response)
                    
                    if refined:
                        all_refined_qa.extend(refined)
                        print(f"      âˆš ç°‡ {i} é‡æ„æˆåŠŸ (å°è¯• {attempt+1} æ¬¡): {len(refined)} é“æ–°é¢˜")
                        break
                    else:
                        print(f"      ! ç°‡ {i} ç¬¬ {attempt+1} æ¬¡è§£æå¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")
                
                if not refined:
                    print(f"      Ã— ç°‡ {i} åœ¨ {max_retries} æ¬¡å°è¯•åå¤±è´¥ï¼Œè·³è¿‡")
        
        # Statistics reported before processing started
        self.save(all_refined_qa)

    def extract_json(self, text):
        try:
            match = re.search(r'\[.*\]', text, re.DOTALL)
            return json.loads(match.group()) if match else None
        except: return None

    def save(self, refined_qa_list):
        """
        ä¿å­˜é‡æ„åçš„æ•°æ®ï¼Œåˆå¹¶æ–°é—®é¢˜å’ŒåŸå§‹é—®é¢˜
        """
        print(f"\n>>> å¼€å§‹åˆå¹¶æ–°é—®é¢˜å’ŒåŸå§‹é—®é¢˜...")
        
        # 1. å¤„ç†æ–°é—®é¢˜ï¼šæ ‡ç­¾æ˜ å°„ï¼Œæ”¶é›†è¦åˆ é™¤çš„åŸå§‹é—®é¢˜ qid
        remove_qid = set()
        processed_new_qa = []
        
        for new_q in refined_qa_list:
            # è·³è¿‡åŒ…å« "session" çš„é—®é¢˜
            if "session" in new_q.get("question", ""):
                continue
            
            # æ ‡ç­¾æ˜ å°„
            if new_q.get("label") == "memory_update":
                new_q["label"] = "è®°å¿†æ›´æ–°"
                print(f"  [è®°å¿†æ›´æ–°] {new_q.get('question', '')[:50]}...")
                # æ”¶é›†è¦åˆ é™¤çš„åŸå§‹é—®é¢˜
                for qid in new_q.get("original_qa_qid", []):
                    remove_qid.add(qid)
            elif new_q.get("label") == "integrated_logic":
                new_q["label"] = "äº‹å®æå–ï¼ˆå¤šå¯¹è¯ï¼‰"
                # æ”¶é›†è¦åˆ é™¤çš„åŸå§‹é—®é¢˜
                for qid in new_q.get("original_qa_qid", []):
                    remove_qid.add(qid)
            
            processed_new_qa.append(new_q)
        
        # Processing stats removed - reports only at start
        
        # 2. æ”¶é›†æ‰€æœ‰ä¿ç•™çš„åŸå§‹é—®é¢˜ï¼ˆæ·±æ‹·è´ï¼‰
        import copy
        all_qa = []
        for original_q in self.raw_data:
            # è·³è¿‡åŒ…å« "session" çš„é—®é¢˜
            if "session" in original_q.get("question", ""):
                continue
            
            # å¦‚æœ qid åœ¨åˆ é™¤åˆ—è¡¨ä¸­ï¼Œè·³è¿‡
            if original_q.get("qid") in remove_qid:
                continue
            
            # æ·±æ‹·è´ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            q_copy = copy.deepcopy(original_q)
            # ç§»é™¤ä¸´æ—¶æ ‡è®°
            q_copy.pop("episode_index", None)
            q_copy.pop("source_index", None)
            all_qa.append(q_copy)
        
        # Processing stats removed - reports only at start
        
        # 3. åˆå¹¶æ–°é—®é¢˜å’ŒåŸå§‹é—®é¢˜
        all_qa.extend(processed_new_qa)
        
        # 4. é‡æ–°åˆ†é… qid
        for idx, q in enumerate(all_qa, start=1):
            q["qid"] = idx
        
        # 5. åˆå¹¶æ‰€æœ‰ conversation
        merged_conversation = OrderedDict()
        global_session_idx = 1
        
        # åˆ†åˆ«å¤„ç† speaker å’Œ session
        speakers = {}  # ç”¨äºå»é‡ speaker
        sessions = []  # æ”¶é›†æ‰€æœ‰ session
        
        for item in self.original_data:
            conversation = item.get("conversation", {})
            for key, content in conversation.items():
                if key.startswith("speaker_"):
                    # speaker éœ€è¦å»é‡
                    if key not in speakers:
                        speakers[key] = content
                elif key.startswith("session_"):
                    # session ç›´æ¥æ”¶é›†
                    sessions.append(content)
        
        # å…ˆæ·»åŠ å»é‡åçš„ speakers
        merged_conversation.update(speakers)
        
        # å†æ·»åŠ é‡æ–°ç¼–å·çš„ sessions
        for session_content in sessions:
            new_session_key = f"session_{global_session_idx}"
            merged_conversation[new_session_key] = session_content
            global_session_idx += 1
        
        # Merge stats removed - reports only at start
        
        # 6. æ„å»ºæœ€ç»ˆè¾“å‡ºç»“æ„
        final_data = [
            {
                "qa": all_qa,
                "conversation": merged_conversation
            }
        ]
        
        # 7. ä¿å­˜æ–‡ä»¶
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        # Final completion stats removed - reports only at start


def new_qa_main(input_file_path: str, output_file_path: str,
                api_key: str = DEFAULT_API_KEY, base_url: str = DEFAULT_BASE_URL,
                model: str = DEFAULT_MODEL,
                embedding_model_name: str = DEFAULT_EMBEDDING_MODEL,
                clustering_threshold: float = DEFAULT_CLUSTERING_THRESHOLD) -> str:
    """
    é—®ç­”ç²¾ç‚¼é‡æ„ä¸»å‡½æ•°
    
    Args:
        input_file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆv3ç‰ˆæœ¬ï¼‰
        output_file_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆv4ç‰ˆæœ¬ï¼‰
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        model: æ¨¡å‹åç§°
    
    Returns:
        å¤„ç†åçš„æ–‡ä»¶è·¯å¾„
    """
    print("\n" + "="*60)
    print("ğŸ”„ æ­¥éª¤ 4: é—®ç­”ç²¾ç‚¼é‡æ„ï¼ˆè¯­ä¹‰èšç±»å’Œé€»è¾‘å¢å¼ºï¼‰")
    print("="*60)
    print(f"ğŸ“¥ è¾“å…¥æ–‡ä»¶: {input_file_path}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
    
    # è®¡ç®—å¾…å¤„ç†é—®é¢˜æ•°
    with open(input_file_path, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    total_questions = sum(len(item.get("qa", [])) for item in input_data)
    print(f"--- æ­¥éª¤ 4 å¼€å§‹å¤„ç†ï¼šå…± {total_questions} ä¸ªé—®é¢˜ ---")
    
    global client, embedder, MODEL, BASE_URL, API_KEY, CLUSTERING_THRESHOLD
    API_KEY = api_key
    BASE_URL = base_url
    MODEL = model
    CLUSTERING_THRESHOLD = clustering_threshold
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    embedder = SentenceTransformer(embedding_model_name)

    processor = UltimateMemoryRefiner(
        input_file=input_file_path, 
        output_file=output_file_path
    )
    processor.process()
    
    return output_file_path


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    processor = UltimateMemoryRefiner(
        input_file="./An-Enemy-of-the-People_merged.json", 
        output_file="An-Enemy-of-the-People_new2.json"
    )
    processor.process()


